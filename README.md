# N-gram-Language-Model
 - Fully implement a foundational statistical model.
 - Deal with live data, including addressing issues of unknown n-grams and words.
 - Create flexible language models, that works for any higher order n grams for n >= 1, both using Laplace smoothing. 
 - Generated sentences from this model using Shannonâ€™s method.
 - Curated an out-of-domain test set of 100 sentences and calculated avg. Probability, Standard Deviation and Perplexity, 

## Dataset
 - Used the Berkeley Restaurant Corpus as my training data. 
 - There are around 7000 sentences in the full corpus.
